{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predição de HRTF usando Random Forest\n\n## Trabalho de Ciência da Computação - 1º Semestre\n\nImplementação baseada no artigo:\n**\"An individualized HRTF model based on random forest and anthropometric parameters\"**  \nTeng & Zhong (2023)\n\n### Objetivos\n- Reproduzir resultados do artigo\n- R² esperado: 90.1% (esquerda) / 91.1% (direita)\n- SD esperado: 4.74 dB\n\n### Base de Dados\n- HUTUBS: 96 sujeitos, 440 posições\n- Frequências: 1-12 kHz (64 bins)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HRTF com Random Forest - Artigo de Teng & Zhong (2023)\n\n## Resumo do Artigo (Abstract)\nRandom Forest para modelar relação não-linear entre parâmetros antropométricos e HRTFs.\nResultados: R² = 90.1%/91.1%, SD = 4.74 dB.\n\n## Motivação (Seção I - Introduction, linhas 56-59)\n\"The relationship between HRTFs and the anthropometric structure is non-linear in essence\"",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## Instalação de Dependências"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# !pip install numpy pandas matplotlib seaborn scikit-learn scipy netCDF4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 1. Configuração"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import os\nimport re\nimport glob\nimport numpy as np\nimport pandas as pd\nimport netCDF4\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 10\nsns.set_style(\"whitegrid\")\n\nnp.random.seed(42)\n\nif os.path.exists('data/hutubs'):\n    root = os.getcwd()\nelse:\n    root = os.path.dirname(os.path.abspath('.'))\n    \ndata_dir = os.path.join(root, \"data\", \"hutubs\")\nprint(f\"Diretório de dados: {data_dir}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 2. Dados Antropométricos\n\n### HUTUBS Database\n96 sujeitos, medidas antropométricas detalhadas.\n\n### Parâmetros (Tabela I do artigo)\n- a1-a5: medidas da cabeça\n- a6-a14: medidas da orelha (d1-d7, θ1-θ2)\n- a15-a19: áreas calculadas"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "csv_file = os.path.join(data_dir, \"AntrhopometricMeasures.csv\")\ndf = pd.read_csv(csv_file)\ndf.columns = [c.strip() for c in df.columns]\ndf[\"SubjectID\"] = df[\"SubjectID\"].astype(int)\n\nprint(f\"Total de sujeitos: {len(df)}\")\nprint(f\"Colunas: {df.columns.tolist()[:15]}...\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 3. Criação dos Parâmetros a1-a19\n\n### Áreas (Seção II.B do artigo)\n- a15 = a6 × a8 / 2 (cavum concha)\n- a16 = a7 × a8 / 2 (cymba concha)  \n- a17 = a9 × a11 / 2 (fossa)\n- a18 = a10 × a11 / 2 (pinna)\n- a19 = a12 × (a6 + a8) / 2 (intertragal)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def create_features_for_ear(df, ear='L'):\n    features = pd.DataFrame()\n    features['SubjectID'] = df['SubjectID']\n    \n    features['a1'] = df['x1']\n    features['a2'] = df['x2']\n    features['a3'] = df['x3']\n    features['a4'] = df['x4']\n    features['a5'] = df['x5']\n    \n    features['a6'] = df[f'{ear}_d1']\n    features['a7'] = df[f'{ear}_d2']\n    features['a8'] = df[f'{ear}_d3']\n    features['a9'] = df[f'{ear}_d4']\n    features['a10'] = df[f'{ear}_d5']\n    features['a11'] = df[f'{ear}_d6']\n    features['a12'] = df[f'{ear}_d7']\n    features['a13'] = df[f'{ear}_theta1']\n    features['a14'] = df[f'{ear}_theta2']\n    \n    features['a15'] = features['a6'] * features['a8'] / 2\n    features['a16'] = features['a7'] * features['a8'] / 2\n    features['a17'] = features['a9'] * features['a11'] / 2\n    features['a18'] = features['a10'] * features['a11'] / 2\n    features['a19'] = features['a12'] * (features['a6'] + features['a8']) / 2\n    \n    return features\n\ndf_left = create_features_for_ear(df, 'L')\ndf_right = create_features_for_ear(df, 'R')\n\ndf_left = df_left.dropna()\ndf_right = df_right.dropna()\n\nprint(f\"Sujeitos com dados completos: {len(df_left)} (esquerda), {len(df_right)} (direita)\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nax = axes[0]\ndf_left[['a1', 'a2', 'a3', 'a4', 'a5']].boxplot(ax=ax)\nax.set_title('Medidas da Cabeça (a1-a5)')\nax.set_ylabel('mm')\n\nax = axes[1]\ndf_left[['a15', 'a16', 'a17', 'a18', 'a19']].boxplot(ax=ax)\nax.set_title('Áreas Calculadas (a15-a19)')\nax.set_ylabel('mm²')\n\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 4. Processamento HRTF\n\n### Especificações (Seção II.B)\n- 5 posições: (0°,0°), (40°,0°), (320°,0°), (0°,30°), (0°,-30°)\n- Frequência: 1-12 kHz (64 bins)\n- Magnitude: 20 × log10|HRTF|"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "fs = 44100\ntgt_freqs = np.linspace(1000, 12000, 64, dtype=np.float32)\npos_tgt = [(0,0), (40,0), (320,0), (0,30), (0,-30)]\n\ncache = {}\n\nprint(\"Processando arquivos SOFA...\")\nfor fp in glob.glob(os.path.join(data_dir, \"pp*_HRIRs_measured.sofa\")):\n    m = re.search(r\"pp(\\d+)_\", fp)\n    if not m:\n        continue\n    sid = int(m.group(1))\n    \n    if sid not in df_left.set_index('SubjectID').index:\n        continue\n    \n    try:\n        ds = netCDF4.Dataset(fp)\n        if \"Data.IR\" not in ds.variables or \"SourcePosition\" not in ds.variables:\n            ds.close()\n            continue\n        \n        ir = ds[\"Data.IR\"][:]\n        pos = ds[\"SourcePosition\"][:, :2]\n        ds.close()\n        \n        if ir.shape[0] < 440:\n            continue\n        \n        orig_f = np.fft.rfftfreq(ir.shape[-1], 1/fs)\n        mask = (orig_f >= 1000) & (orig_f <= 12000)\n        band_f = orig_f[mask]\n        \n        idx_map = [np.argmin(np.sum((pos - p)**2, axis=1)) for p in pos_tgt]\n        \n        for ear in (0,1):\n            mag = np.abs(np.fft.rfft(ir[:, ear, :], axis=-1)) + 1e-10\n            mag_db = 20*np.log10(mag[:, mask])\n            \n            for k, pidx in enumerate(idx_map):\n                cache[(sid, ear, k)] = np.interp(\n                    tgt_freqs, band_f, mag_db[pidx]).astype(np.float32)\n    except Exception as e:\n        continue\n\nvalid_sids = sorted({sid for (sid,_,_) in cache})\nprint(f\"Sujeitos válidos: {len(valid_sids)}\")\n\nexclude = [18, 56, 79, 80, 92, 94]\nvalid_sids = [s for s in valid_sids if s not in exclude]\nprint(f\"Após exclusões: {len(valid_sids)} sujeitos\")\n\nvalid_sids = np.array(valid_sids, dtype=int)\nnp.random.shuffle(valid_sids)\ntest_sub = valid_sids[:10]\ntrain_sub = valid_sids[10:]\n\nprint(f\"Divisão: {len(train_sub)} treino, {len(test_sub)} teste\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 5. Preparação dos Dados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def build_dataset(sub_ids, ear, pos_id):\n    X, Y = [], []\n    \n    df_ear = df_left if ear == 0 else df_right\n    df_ear = df_ear.set_index('SubjectID')\n    \n    for sid in sub_ids:\n        key = (sid, ear, pos_id)\n        if key not in cache or sid not in df_ear.index:\n            continue\n        \n        spec = cache[key]\n        feats = df_ear.loc[sid, ['a'+str(i) for i in range(1,20)]].to_numpy(dtype=np.float32)\n        \n        rep = np.repeat(feats.reshape(1,-1), 64, axis=0)\n        X.append(np.hstack([rep, tgt_freqs.reshape(-1,1)]))\n        Y.append(spec)\n    \n    if not X:\n        return None, None\n    \n    return np.vstack(X), np.hstack(Y)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 6. Treinamento Random Forest\n\n### Configuração (Seção II.A)\n- 500 árvores\n- 18 variáveis selecionadas (max_features)\n- min_samples_leaf=5 (padrão MATLAB)\n- OOB scoring"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def r2_corr(y, yhat):\n    r = np.corrcoef(y, yhat)[0,1]\n    return (r*r) if not np.isnan(r) else 0.0\n\nmax_feat = 18\n\nR2 = {0:[], 1:[]}\nSD = {0:[], 1:[]}\nmodels = {}\n\nprint(\"\\nTreinando modelos...\")\n\nfor ear in (0, 1):\n    for pid in range(5):\n        X_tr, y_tr = build_dataset(train_sub, ear, pid)\n        if X_tr is None:\n            R2[ear].append(0.0)\n            SD[ear].append(np.nan)\n            continue\n        \n        rf = RandomForestRegressor(\n            n_estimators=500,\n            max_features=max_feat,\n            min_samples_split=2,\n            min_samples_leaf=5,\n            bootstrap=True,\n            oob_score=True,\n            n_jobs=-1,\n            random_state=42\n        )\n        \n        rf.fit(X_tr, y_tr)\n        R2[ear].append(r2_corr(y_tr, rf.oob_prediction_))\n        \n        X_te, y_te = build_dataset(test_sub, ear, pid)\n        if X_te is not None:\n            y_pred = rf.predict(X_te)\n            sd_value = np.sqrt(np.mean((y_pred - y_te)**2))\n            SD[ear].append(sd_value)\n        else:\n            SD[ear].append(np.nan)\n        \n        models[(ear, pid)] = rf\n\nprint(\"Treinamento concluído!\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 7. Resultados - Tabela II do Artigo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "hdr = [\"Ear\", \"(0°,0°)\", \"(40°,0°)\", \"(320°,0°)\", \"(0°,30°)\", \"(0°,-30°)\", \"Mean\"]\n\nprint(\"\\nTABLE II – Determination Coefficients R²\")\nprint(\"=\"*60)\nprint(\" | \".join(hdr))\nprint(\"-\"*60)\n\nfor ear, label in ((0, \"Left\"), (1, \"Right\")):\n    row = [f\"{v*100:5.1f}%\" for v in R2[ear]]\n    mean_r2 = np.nanmean(R2[ear])\n    print(\" | \".join([label] + row + [f\"{mean_r2*100:5.1f}%\"]))\n\nprint(\"\\n\\nSpectral Distortion SD (dB)\")\nprint(\"=\"*60)\nprint(\" | \".join(hdr))\nprint(\"-\"*60)\n\nfor ear, label in ((0, \"Left\"), (1, \"Right\")):\n    row = [f\"{v:5.2f}\" for v in SD[ear]]\n    mean_sd = np.nanmean(SD[ear])\n    print(\" | \".join([label] + row + [f\"{mean_sd:5.2f}\"]))\n\nprint(f\"\\n\\nComparação com artigo:\")\nprint(f\"Artigo: R² = 90.1% (left), 91.1% (right), SD = 4.74 dB\")\nprint(f\"Nosso: R² = {np.nanmean(R2[0])*100:.1f}% (left), {np.nanmean(R2[1])*100:.1f}% (right), SD = {np.mean([np.nanmean(SD[0]), np.nanmean(SD[1])]):.2f} dB\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 8. Importância das Features\n\n### Seção III.A do artigo\nParâmetros importantes: a4, a14, a16, a19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "feature_names = ['a'+str(i) for i in range(1,20)] + ['frequency']\nimportances = np.zeros(20)\nn_models = 0\n\nfor (ear, pid), model in models.items():\n    importances += model.feature_importances_\n    n_models += 1\n\nimportances /= n_models\n\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nimportant_features = ['a4', 'a14', 'a16', 'a19']\ncolors = ['red' if f in important_features else 'blue' for f in importance_df['feature'][:15]]\nplt.barh(importance_df['feature'][:15], importance_df['importance'][:15], color=colors)\nplt.xlabel('Importância')\nplt.title('Features Mais Importantes')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(\"Top 10 features:\")\nfor i, row in importance_df.head(10).iterrows():\n    mark = \" ***\" if row['feature'] in important_features else \"\"\n    print(f\"{row['feature']}: {row['importance']:.4f}{mark}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 9. Erro por Frequência\n\n### Seção III.B\nSD aumenta com frequência > 6 kHz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "errors_by_freq = {0: [], 1: []}\n\nfor ear in (0, 1):\n    for pid in range(5):\n        X_te, y_te = build_dataset(test_sub, ear, pid)\n        if X_te is None or (ear, pid) not in models:\n            continue\n        \n        model = models[(ear, pid)]\n        y_pred = model.predict(X_te)\n        \n        n_subjects = len(test_sub)\n        errors = np.abs(y_pred - y_te).reshape(n_subjects, -1)\n        errors_by_freq[ear].append(errors.mean(axis=0))\n\nplt.figure(figsize=(10, 5))\n\nfor ear, label in ((0, 'Esquerda'), (1, 'Direita')):\n    if errors_by_freq[ear]:\n        mean_error = np.mean(errors_by_freq[ear], axis=0)\n        plt.plot(tgt_freqs/1000, mean_error, label=f'Orelha {label}', linewidth=2)\n\nplt.axvline(x=6, color='red', linestyle='--', alpha=0.5, label='6 kHz')\nplt.xlabel('Frequência (kHz)')\nplt.ylabel('Erro Médio (dB)')\nplt.title('Erro vs Frequência')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 10. Desempenho por Posição"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\npositions = ['(0°,0°)', '(40°,0°)', '(320°,0°)', '(0°,30°)', '(0°,-30°)']\nx = np.arange(len(positions))\nwidth = 0.35\n\nax1.bar(x - width/2, [r*100 for r in R2[0]], width, label='Esquerda')\nax1.bar(x + width/2, [r*100 for r in R2[1]], width, label='Direita')\nax1.set_xlabel('Posição')\nax1.set_ylabel('R² (%)')\nax1.set_title('R² por Posição')\nax1.set_xticks(x)\nax1.set_xticklabels(positions, rotation=45)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.bar(x - width/2, SD[0], width, label='Esquerda')\nax2.bar(x + width/2, SD[1], width, label='Direita')\nax2.set_xlabel('Posição')\nax2.set_ylabel('SD (dB)')\nax2.set_title('Distorção por Posição')\nax2.set_xticks(x)\nax2.set_xticklabels(positions, rotation=45)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 11. Curva de Aprendizado"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "X_sample, y_sample = build_dataset(train_sub, ear=0, pos_id=0)\n\nif X_sample is not None:\n    train_sizes = np.linspace(0.1, 1.0, 10)\n    train_sizes_abs, train_scores, val_scores = learning_curve(\n        RandomForestRegressor(n_estimators=100, max_features=18, min_samples_leaf=5, random_state=42),\n        X_sample, y_sample, \n        train_sizes=train_sizes,\n        cv=5,\n        scoring='r2',\n        n_jobs=-1\n    )\n    \n    plt.figure(figsize=(8, 5))\n    plt.plot(train_sizes_abs, train_scores.mean(axis=1), 'o-', label='Treino')\n    plt.plot(train_sizes_abs, val_scores.mean(axis=1), 'o-', label='Validação')\n    plt.xlabel('Amostras de Treino')\n    plt.ylabel('R²')\n    plt.title('Curva de Aprendizado')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 12. Matriz de Correlação"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "feature_cols = ['a' + str(i) for i in range(1, 20)]\ncorr_matrix = df_left[feature_cols].corr()\n\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', center=0,\n            square=True, cbar_kws={\"shrink\": .8})\nplt.title('Correlação entre Parâmetros')\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 13. Distribuição dos Erros"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "all_errors = []\n\nfor ear in (0, 1):\n    for pid in range(5):\n        X_te, y_te = build_dataset(test_sub, ear, pid)\n        if X_te is None or (ear, pid) not in models:\n            continue\n        \n        model = models[(ear, pid)]\n        y_pred = model.predict(X_te)\n        errors = y_pred - y_te\n        all_errors.extend(errors)\n\nall_errors = np.array(all_errors)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nax = axes[0]\nax.hist(all_errors, bins=50, density=True, alpha=0.7, edgecolor='black')\nax.set_xlabel('Erro (dB)')\nax.set_ylabel('Densidade')\nax.set_title('Distribuição dos Erros')\n\nfrom scipy import stats\nax = axes[1]\nstats.probplot(all_errors, dist=\"norm\", plot=ax)\nax.set_title('Q-Q Plot')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estatísticas dos erros:\")\nprint(f\"Média: {all_errors.mean():.3f} dB\")\nprint(f\"Desvio: {all_errors.std():.3f} dB\")\nprint(f\"Mediana: {np.median(all_errors):.3f} dB\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 14. Exemplo de Predição"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "test_subject = test_sub[0]\near = 0\npos_id = 0\n\nX_ex, y_ex = build_dataset([test_subject], ear, pos_id)\n\nif X_ex is not None and (ear, pos_id) in models:\n    model = models[(ear, pos_id)]\n    y_pred = model.predict(X_ex)\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(tgt_freqs/1000, y_ex, 'b-', linewidth=2, label='HRTF Real')\n    plt.plot(tgt_freqs/1000, y_pred, 'r--', linewidth=2, label='HRTF Predita')\n    plt.xlabel('Frequência (kHz)')\n    plt.ylabel('Magnitude (dB)')\n    plt.title(f'Sujeito {test_subject}, Posição (0°,0°)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    r2 = r2_score(y_ex, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_ex, y_pred))\n    print(f\"R²: {r2*100:.1f}%\")\n    print(f\"RMSE: {rmse:.2f} dB\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## 15. Conclusões"
  },
  {
   "cell_type": "code",
   "source": "print(\"RESULTADOS FINAIS\")\nprint(\"=\"*50)\nprint(\"\\nArtigo (Teng & Zhong 2023):\")\nprint(\"R² = 90.1% (esquerda), 91.1% (direita)\")\nprint(\"SD = 4.74 dB\")\n\nprint(f\"\\nNossa implementação:\")\nprint(f\"R² = {np.nanmean(R2[0])*100:.1f}% (esquerda), {np.nanmean(R2[1])*100:.1f}% (direita)\")\nprint(f\"SD = {np.mean([np.nanmean(SD[0]), np.nanmean(SD[1])]):.2f} dB\")\n\nprint(\"\\n\\nPONTOS CHAVE:\")\nprint(\"1. Parâmetros a1-a19 conforme Tabela I\")\nprint(\"2. max_features=18, min_samples_leaf=5\")\nprint(\"3. Features importantes: a4, a14, a16, a19\")\nprint(\"4. Erro aumenta com frequência > 6 kHz\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}